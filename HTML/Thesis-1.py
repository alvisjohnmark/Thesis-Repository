# -*- coding: utf-8 -*-
"""thesis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1guek7hcD1FUHGWupyYLDC5r7w1VEm2Db
"""

# Importing required libraries
# Keras
# sklearn
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Other
import librosa
import librosa.display
import json
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import specgram
import pandas as pd
import seaborn as sns
import glob
import os
import pickle
import IPython.display as ipd  # To play sound in the notebook
from google.colab import drive
from sklearn.preprocessing import MinMaxScaler, StandardScaler
drive.mount('/content/drive/', force_remount=True)

"""# Data exploration"""

path_audio = '/content/drive/MyDrive/thesis/audio/'
data = '/content/drive/MyDrive/thesis/audio_dataset.csv'
# data = {
#     'path': [path_audio + str(i) + '.wav' for i in range(1, 21)],
#     'label': [0,1,2,3,0,1,2,3,0,1,2,3,0,1,2,3,0,1,2,3]
# }
data = pd.read_csv(data)
df = pd.DataFrame(data)

def createPath(code):
  return path_audio + str(code) + '.mp3'
df['path'] = df['Code'].apply(createPath)
df.drop('Code', axis=1, inplace=True)

ipd.Audio(filename=df['path'][2])

lb = LabelEncoder()
d = lb.fit_transform(df['Label'])
df['label'] = d
df.drop('Label', axis=1, inplace=True)
df

classes_dict = {
    0: 'Happy',
    1: 'Angry',
    2: 'Sad',
    3: 'Calm'
}

p = df['label'].value_counts()
classes = list(["Happy", "Angry", "Sad", "Calm"])
values = list(p)

fig = plt.figure(figsize = (10, 5))

plt.bar(classes, values, color ='green', width = 0.4)

plt.xlabel("Emotion")
plt.ylabel("No. of adio")
plt.title("Number of songs by emotion")
plt.show()

for c in classes_dict.keys():
  path = df[df['label'] == c]['path'].iloc[0]
  sample_rate = librosa.get_samplerate(path)
  signal, rate = librosa.load(path, sr=sample_rate)
  plt.figure(figsize=(20, 5))
  librosa.display.waveshow(signal, sr=rate, alpha=0.5)
  plt.title(classes_dict[c])
  plt.ylabel('Amplitude')
  plt.xlabel('Time')
  plt.show()

for c in classes_dict.keys():
  path = df[df['label'] == c]['path'].iloc[0]
  sample_rate = librosa.get_samplerate(path)
  signal, rate = librosa.load(path, sr=sample_rate)
  plt.figure(figsize=(20, 5))
  ft = np.abs(librosa.stft(signal))
  plt.title(classes_dict[c])
  plt.xlabel('Frequeny')
  plt.ylabel('Magnitude')
  plt.plot(ft);

FRAME_SIZE = 1024
HOP_LENGTH = 512

for c in classes_dict.keys():
  path = df[df['label'] == c]['path'].iloc[0]
  sample_rate = librosa.get_samplerate(path)
  signal, rate = librosa.load(path, sr=sample_rate)
  plt.figure(figsize=(20, 5))
  ft = np.abs(librosa.stft(signal))
  DB = librosa.amplitude_to_db(ft, ref = np.max)
  plt.figure(figsize = (15, 5))
  librosa.display.specshow(DB, sr = rate, hop_length = HOP_LENGTH, x_axis = 'time', y_axis = 'log',cmap = 'cool')
  plt.colorbar();
  plt.title(classes_dict[c]);

FRAME_SIZE = 1024
HOP_LENGTH = 512

for c in classes_dict.keys():
  path = df[df['label'] == c]['path'].iloc[0]
  sample_rate = librosa.get_samplerate(path)
  signal, rate = librosa.load(path, sr=sample_rate)
  plt.figure(figsize=(20, 5))
  S = librosa.feature.melspectrogram(y=signal, sr=rate)
  S_DB = librosa.amplitude_to_db(S, ref=np.max)
  plt.figure(figsize = (16, 6))
  librosa.display.specshow(S_DB, sr=rate, hop_length=HOP_LENGTH, x_axis = 'time', y_axis = 'log', cmap = 'cool');
  plt.colorbar();
  plt.title("Mel Spectrogram: " + classes_dict[c]);

FRAME_SIZE = 1024
HOP_LENGTH = 512


def amplitude_envelope(signal, frame_size, hop_length):

    amplitude_envelope = []

    # calculate amplitude envelope for each frame
    for i in range(0, len(signal), hop_length):
        amplitude_envelope_current_frame = max(signal[i:i+frame_size])
        amplitude_envelope.append(amplitude_envelope_current_frame)

    return np.array(amplitude_envelope)



for c in classes_dict.keys():
  path = df[df['label'] == c]['path'].iloc[0]
  sample_rate = librosa.get_samplerate(path)
  signal, rate = librosa.load(path, sr=sample_rate)
  ae = amplitude_envelope(signal, frame_size=FRAME_SIZE, hop_length=HOP_LENGTH)
  plt.figure(figsize=(20, 5))
  frames = range(len(ae))
  t = librosa.frames_to_time(frames, hop_length=HOP_LENGTH)
  librosa.display.waveshow(signal, alpha=0.5)
  plt.plot(t, ae, color='red')
  plt.title(classes_dict[c])

happy = df[df['label'] == 0].iloc[0]['path']
angry = df[df['label'] == 1].iloc[0]['path']
sad = df[df['label'] == 2].iloc[0]['path']
calm = df[df['label'] == 3].iloc[0]['path']

sample = [happy, angry, sad, calm]
ys = []

for i , path in enumerate(sample):
  sample_rate = librosa.get_samplerate(path)
  y, sr = librosa.load(path, res_type='soxr_lq',duration=25 ,sr=sample_rate * 2 ,offset=3)
  mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
  print(mfccs.shape)
  plt.figure(figsize=(10, 6))
  librosa.display.specshow(mfccs, x_axis='time', sr=sr, cmap='coolwarm')
  plt.colorbar(format="%+2.0f dB")
  plt.title(f"MFCCs for {i}")
  plt.xlabel("Time (s)")
  plt.ylabel("MFCC Coefficients")
  plt.show()

happy = df[df['label'] == 0].iloc[0]['path']
angry = df[df['label'] == 1].iloc[0]['path']
sad = df[df['label'] == 2].iloc[0]['path']
calm = df[df['label'] == 3].iloc[0]['path']

y1, sr = librosa.load(happy, res_type='soxr_lq',duration=25 ,sr=22050 * 2 ,offset=3)

y2, sr = librosa.load(angry, res_type='soxr_lq',duration=25 ,sr=22050 * 2 ,offset=3)

y3, sr = librosa.load(sad, res_type='soxr_lq',duration=25 ,sr=22050 * 2 ,offset=3)

y4, sr = librosa.load(calm, res_type='soxr_lq',duration=25 ,sr=22050 * 2 ,offset=3)

fig, ax = plt.subplots(nrows=5, sharex=True, figsize=(12,7))
fig.tight_layout()
librosa.display.waveshow(y1, sr=sr, ax=ax[0])
ax[0].set(title='Happy')
ax[0].label_outer()

librosa.display.waveshow(y2, sr=sr, ax=ax[1])
ax[1].set(title='Angry')
ax[1].label_outer()

librosa.display.waveshow(y3, sr=sr, ax=ax[2])
ax[2].set(title='Sad')
ax[2].label_outer()

librosa.display.waveshow(y4, sr=sr, ax=ax[3])
ax[3].set(title='Calm')
ax[3].label_outer()

librosa.display.waveshow(y1, sr=sr, alpha=0.5, ax=ax[4], label='Happy')
librosa.display.waveshow(y2, sr=sr, color='r', alpha=0.5, ax=ax[4], label='Angry')
librosa.display.waveshow(y3, sr=sr, color='g', alpha=0.5, ax=ax[4], label='Sad')
librosa.display.waveshow(y4, sr=sr, color='y', alpha=0.5, ax=ax[4], label='Calm')

ax[4].set(title='Multiple waveforms')
ax[4].legend()

# y, sr = librosa.load("your_audio_file.wav")
# S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
# S_dB = librosa.power_to_db(S, ref=np.max)  # Convert to dB scale for perceptual intensity

# 1. Compute threshold: keep only top 25% intensity values
# threshold = np.percentile(y1, 25)
# threshold = np.mean(y1)
# print(threshold)
positive_mean = np.percentile(y1[y1 >= 0], 90)
negative_mean = np.percentile(y1[y1 < 0], 10)

# 2. Mask out values below the threshold
dominant_S = np.array([
    val if (val >= positive_mean or val <= negative_mean) else 0.0
    for val in y1
])

# dominant_S = np.where(y1 >= threshold, y1, 0)

# 3. Plot original vs. filtered spectrogram
plt.figure(figsize=(14, 7))

librosa.display.waveshow(y1, sr=sr, alpha=1)
plt.axhline(y=positive_mean, color='r', linestyle='--', label='Positive Threshold')
plt.axhline(y=negative_mean, color='b', linestyle='--', label='Negative Threshold')

librosa.display.waveshow(dominant_S, sr=sr, alpha=0.5)

# plt.subplot(1, 2, 2)
# librosa.display.waveshow(y1, sr=sr)
# plt.title('Original Mel Spectrogram')

# plt.subplot(1, 2, 2)
# librosa.display.waveshow(dominant_S, sr=sr)
# plt.title('Dominant Regions Only')

plt.tight_layout()
plt.show()

# Get Mel Spectrogram
S = librosa.feature.melspectrogram(y=dominant_S, sr=sr, n_mels=128)
S_dB = librosa.power_to_db(S, ref=np.max)

# Compute thresholds
high_threshold = np.percentile(S_dB, 75)   # High energy regions
low_threshold = np.percentile(S_dB, 25)    # Low energy regions

# Create masks
high_energy_mask = S_dB >= high_threshold
low_energy_mask = S_dB <= low_threshold

# Combine both masks
dominant_mask = high_energy_mask | low_energy_mask

# Apply mask
selected_S = np.where(dominant_mask, S_dB, 0)

# Plot
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')
plt.title('Original Spectrogram')
plt.colorbar(format='%+2.0f dB')

plt.subplot(1, 2, 2)
librosa.display.specshow(selected_S, sr=sr, x_axis='time', y_axis='mel')
plt.title('High & Low Amplitudes Only')
plt.colorbar(format='%+2.0f dB')

plt.tight_layout()
plt.show()

positive_mean = np.percentile(y4[y4 >= 0], 90)
negative_mean = np.percentile(y4[y4 < 0], 10)

# positive_mean = np.mean(y4[y4 >= 0])
# negative_mean = np.mean(y4[y4 < 0])

# 2. Mask out values below the threshold
dominant_S = np.array([
    val if (val <= positive_mean and val >= negative_mean) else 0.0
    for val in y4
])
# Keep only low-amplitude values
# y_low_amp = np.where((y >= low_neg_threshold) & (y <= low_pos_threshold), y, 0)


# dominant_S = np.where(y1 >= threshold, y1, 0)

# 3. Plot original vs. filtered spectrogram
plt.figure(figsize=(14, 7))

librosa.display.waveshow(y4, sr=sr, alpha=1)
plt.axhline(y=positive_mean, color='r', linestyle='--', label='Positive Threshold')
plt.axhline(y=negative_mean, color='b', linestyle='--', label='Negative Threshold')

librosa.display.waveshow(dominant_S, sr=sr, alpha=0.5)

# plt.subplot(1, 2, 2)
# librosa.display.waveshow(y1, sr=sr)
# plt.title('Original Mel Spectrogram')

# plt.subplot(1, 2, 2)
# librosa.display.waveshow(dominant_S, sr=sr)
# plt.title('Dominant Regions Only')

plt.tight_layout()
plt.show()

# Get Mel Spectrogram
S = librosa.feature.melspectrogram(y=y1, sr=sr, n_mels=128)
S_dB = librosa.power_to_db(S, ref=np.max)

# Compute thresholds
high_threshold = np.percentile(S_dB, 75)   # High energy regions
low_threshold = np.percentile(S_dB, 25)    # Low energy regions

# Create masks
high_energy_mask = S_dB >= high_threshold
low_energy_mask = S_dB <= low_threshold

# Combine both masks
dominant_mask = high_energy_mask | low_energy_mask

# Apply mask
selected_S = np.where(dominant_mask, S_dB, 0)

# Plot
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')
plt.title('Original Spectrogram')
plt.colorbar(format='%+2.0f dB')

plt.subplot(1, 2, 2)
librosa.display.specshow(selected_S, sr=sr, x_axis='time', y_axis='mel')
plt.title('High & Low Amplitudes Only')
plt.colorbar(format='%+2.0f dB')

plt.tight_layout()
plt.show()

# Get Mel Spectrogram
S = librosa.feature.melspectrogram(y=y4, sr=sr, n_mels=128)
S_dB = librosa.power_to_db(S, ref=np.max)

# Compute thresholds
high_threshold = np.percentile(S_dB, 75)   # High energy regions
low_threshold = np.percentile(S_dB, 25)    # Low energy regions

# Create masks
# high_energy_mask = S_dB >= high_threshold
# low_energy_mask = S_dB <= low_threshold

# Combine both masks
dominant_mask = (S_dB >= low_threshold) & (S_dB <= high_threshold)

# Apply mask
selected_S = np.where(dominant_mask, S_dB, 0)

# Plot
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')
plt.title('Original Spectrogram')
plt.colorbar(format='%+2.0f dB')

plt.subplot(1, 2, 2)
librosa.display.specshow(selected_S, sr=sr, x_axis='time', y_axis='mel')
plt.title('High & Low Amplitudes Only')
plt.colorbar(format='%+2.0f dB')

plt.tight_layout()
plt.show()

dominant_S

zcr1 = librosa.feature.zero_crossing_rate(y1)
zcr3 = librosa.feature.zero_crossing_rate(y2)

plt.figure(figsize=(12, 4))
plt.plot(zcr1[0], label='Line 1', color='blue')
plt.plot(zcr3[0], label='Line 2', color='orange')
plt.title('Line Plot of 2000 Elements')
plt.xlabel('Index')
plt.ylabel('Value')
plt.grid(True)
plt.tight_layout()
plt.show()

rms1 = librosa.feature.rms(y=y1)
rms3 = librosa.feature.rms(y=y2)

plt.figure(figsize=(12, 4))
plt.plot(rms1[0], label='Line 1', color='blue')
plt.plot(rms3[0], label='Line 2', color='orange')
plt.title('Line Plot of 2000 Elements')
plt.xlabel('Index')
plt.ylabel('Value')
plt.grid(True)
plt.tight_layout()
plt.show()

count_lower = np.sum(rms1[0] < np.mean(rms1[0]))
count_higher = np.sum(rms1[0] >= np.mean(rms1[0]))
print(count_lower)
print(count_higher)

plt.figure(figsize=(10, 5))
plt.hist(rms1[0], bins=50, alpha=0.5, label='Data 1', color='blue', density=True)
plt.hist(rms3[0], bins=50, alpha=0.5, label='Data 2', color='orange', density=True)

plt.title('Histogram of Two Arrays')
plt.xlabel('Value')
plt.ylabel('Frequency (normalized)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))

# Plot KDEs
sns.kdeplot(rms1[0], fill=True, color='skyblue', label='Array 1', linewidth=2)
sns.kdeplot(rms3[0], fill=True, color='orange', label='Array 2', linewidth=2)

# Mean and percentiles for rms1[0]
mean1 = np.mean(rms1[0])
p25_1 = np.percentile(rms1[0], 25)
p75_1 = np.percentile(rms1[0], 75)

plt.axvline(mean1, color='blue', linestyle='--', linewidth=1, label='Array 1 Mean')
plt.axvline(p25_1, color='blue', linestyle=':', linewidth=1, label='Array 1 25th %ile')
plt.axvline(p75_1, color='blue', linestyle=':', linewidth=1, label='Array 1 75th %ile')

# Mean and percentiles for rms3[0]
mean2 = np.mean(rms3[0])
p25_2 = np.percentile(rms3[0], 25)
p75_2 = np.percentile(rms3[0], 75)

plt.axvline(mean2, color='darkorange', linestyle='--', linewidth=1, label='Array 2 Mean')
plt.axvline(p25_2, color='darkorange', linestyle=':', linewidth=1, label='Array 2 25th %ile')
plt.axvline(p75_2, color='darkorange', linestyle=':', linewidth=1, label='Array 2 75th %ile')

plt.title('Density Plot with Overlays')
plt.xlabel('Value')
plt.ylabel('Density')
plt.legend()
plt.grid(True)
plt.show()

from scipy.signal import find_peaks
# Plot the KDE again
plt.figure(figsize=(10, 6))
sns.kdeplot(rms1[0], fill=True, color='skyblue', linewidth=2, label='KDE')

# Extract x and y from the KDE plot
kde = sns.kdeplot(rms1[0])
x_vals, y_vals = kde.get_lines()[0].get_data()
plt.clf()  # Clear the background duplicate plot

# Find peaks
peaks, _ = find_peaks(y_vals)
dominant_x = x_vals[peaks]
dominant_y = y_vals[peaks]

# Plot again for visualization
plt.figure(figsize=(10, 6))
sns.kdeplot(rms1[0], fill=True, color='skyblue', linewidth=2, label='KDE')
plt.scatter(dominant_x, dominant_y, color='red', zorder=5, label='Peaks')
plt.vlines(dominant_x, ymin=0, ymax=dominant_y, colors='red', linestyles='--', alpha=0.6)

# Labels and legend
plt.title("KDE with Dominant Values (Peaks)")
plt.xlabel("Value")
plt.ylabel("Density")
plt.legend()
plt.grid(True)
plt.show()

import arviz as az

# Example array (replace with your actual data)
# rms3[0] = ...

# Compute HDI bounds
hdi_bounds = az.hdi(rms1[0], hdi_prob=0.90)
lower, upper = hdi_bounds[0], hdi_bounds[1]

# Plot KDE
plt.figure(figsize=(10, 6))
sns.kdeplot(rms1[0], fill=True, color='skyblue', linewidth=2, label='KDE')

# Highlight HDI region
plt.axvline(lower, color='green', linestyle='--', label=f'Lower 90% HDI: {lower:.4f}')
plt.axvline(upper, color='green', linestyle='--', label=f'Upper 90% HDI: {upper:.4f}')
plt.fill_betweenx([0, plt.ylim()[1]], lower, upper, color='green', alpha=0.2, label='90% HDI Region')

# Labels and legend
plt.title('KDE with 90% Highest Density Interval (HDI)')
plt.xlabel('Value')
plt.ylabel('Density')
plt.legend()
plt.grid(True)
plt.show()



from scipy import stats

print(stats.mode(zcr1[0]))
print(np.mean(zcr[0]))

stats.mode(zcr3[0])

mfccs1 = librosa.feature.mfcc(y=y1, sr=sr, n_mfcc=40)
mfccs3 = librosa.feature.mfcc(y=y3, sr=sr, n_mfcc=40)

hdi_prob = 0.9
# Compute HDI for each MFCC row
hdi_bounds = np.array([az.hdi(row, hdi_prob=hdi_prob) for row in mfccs1])

mask = np.zeros_like(mfccs1, dtype=bool) # Initialize with False

# Apply HDI to create the mask
for i in range(mfccs1.shape[0]):
    lower, upper = hdi_bounds[i]
    mask[i, :] = (mfccs1[i, :] >= lower) & (mfccs1[i, :] <= upper)


# Display MFCCs
plt.figure(figsize=(12, 6))
plt.subplot(2, 1, 2)
librosa.display.specshow(mask, x_axis='time', sr=sr)

# plt.subplot(2, 1, 2)
# librosa.display.specshow(mfccs3, x_axis='time', sr=sr)
plt.colorbar(format='%+2.0f dB')
plt.title('MFCC Coefficients')
plt.tight_layout()
plt.show()
mfccs1.shape

# masked_mfcc = []

# for i in range(mfccs1.shape[0]):
#     lower, upper = hdi_bounds[i]
#     mask = (mfccs1[i] >= lower) & (mfccs1[i] <= upper)
#     masked_values = mfccs1[i][mask]
#     masked_mfcc.append(np.mean(masked_values))
# masked_mfcc

mfccs1 = librosa.feature.mfcc(y=y1, sr=sr, n_mfcc=40)
# Display MFCCs
plt.figure(figsize=(12, 6))
plt.subplot(2, 1, 2)
librosa.display.specshow(mfccs1, x_axis='time', sr=sr)

# plt.subplot(2, 1, 2)
# librosa.display.specshow(mfccs3, x_axis='time', sr=sr)
plt.colorbar(format='%+2.0f dB')
plt.title('MFCC Coefficients')
plt.tight_layout()
plt.show()

S = np.abs(librosa.stft(y3))
amp = librosa.amplitude_to_db(S)[0]

hdi_bounds = az.hdi(amp, hdi_prob=0.90)
lower, upper = hdi_bounds[0], hdi_bounds[1]

# Plot KDE
plt.figure(figsize=(10, 6))
sns.kdeplot(amp, fill=True, color='skyblue', linewidth=2, label='KDE')

# Highlight HDI region
plt.axvline(lower, color='green', linestyle='--', label=f'Lower 90% HDI: {lower:.4f}')
plt.axvline(upper, color='green', linestyle='--', label=f'Upper 90% HDI: {upper:.4f}')
plt.fill_betweenx([0, plt.ylim()[1]], lower, upper, color='green', alpha=0.2, label='90% HDI Region')

# Labels and legend
plt.title('KDE with 90% Highest Density Interval (HDI)')
plt.xlabel('Value')
plt.ylabel('Density')
plt.legend()
plt.grid(True)
plt.show()
vals = (amp >= lower) & (amp[i] <= upper)
print(np.mean(amp[vals]))

for i , path in enumerate(sample):
  sample_rate = librosa.get_samplerate(path)
  y, sr = librosa.load(path, res_type='soxr_lq',duration=25 ,sr=sample_rate * 2 ,offset=3)
  mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
  plt.figure(figsize=(10, 6))
  librosa.display.specshow(mfccs, x_axis='time', sr=sr, cmap='coolwarm')
  plt.colorbar(format="%+2.0f dB")
  plt.title(f"MFCCs for {i}")
  plt.xlabel("Time (s)")
  plt.ylabel("MFCC Coefficients")
  plt.show()

df

import os.path
def isFileExists(path):
  try:
    y, sr = librosa.load(path, res_type='soxr_lq',duration=1 ,sr=22050 ,offset=5)
  except:
    df.drop(df[df['path'] == path].index, inplace=True)

df.shape

"""# Feature extraction

## Extract by feature
"""

df_features = pd.DataFrame(columns=['feature'])


counter = 0
def appendResults(path):
  try:
    y, sr = librosa.load(path, duration=25 ,sr=22050*2 ,offset=3)
    y, _ = librosa.effects.trim(y)
    bpm = extractBPM(y, sr)
    mfccs = extractMFCC(y, sr)
    rms = extractRMS(y, sr)
    zcr = extractZCR(y, sr)
    chroma_stft = extractChromaSTFT(y, sr)
    spectral_centroid = extractSpectralCentroid(y, sr)
    spectral_bandwidth = extractSpectralBandwidth(y, sr)
    spectral_rolloff = extractSpectralRolloff(y, sr)
    spectral_contrast = extractSpectralContrast(y, sr)
    mel_spectogram = extractMelSpectogram(y, sr)
    amplitude = extractAmplitude(y, sr)
  except:
    df.drop(df[df['path'] == path].index, inplace=True)
    return None
  return [bpm, mfccs, rms, zcr, chroma_stft, spectral_centroid, spectral_bandwidth, spectral_rolloff, spectral_contrast, mel_spectogram, amplitude]

def extractAmplitude(y, sr):
  S = np.abs(librosa.stft(y))
  amp = librosa.amplitude_to_db(S)
  print("amp: ", len(amp))
  return amp

def extractMFCC(y, sr):
  mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)
  print("mfccs: ", len(mfccs))
  return mfccs

def extractRMS(y, sr):
  rms = librosa.feature.rms(y=y)
  print("rms: ", len(rms))
  return rms

def extractZCR(y, sr):
  zcr = librosa.feature.zero_crossing_rate(y)
  print("zcr: ", len(zcr))
  return zcr

def extractChromaSTFT(y, sr):
  chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)
  print("chroma_stf: ", len(chroma_stft))
  return chroma_stft

def extractSpectralCentroid(y, sr):
  cent = librosa.feature.spectral_centroid(y=y, sr=sr)
  print("cent: ", len(cent))
  return cent

def extractSpectralBandwidth(y, sr):
  spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)
  print("spec_bw: ", len(spec_bw))
  return spec_bw

def extractSpectralRolloff(y, sr):
  rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
  print("rolloff: ", len(rolloff))
  return rolloff

def extractSpectralContrast(y, sr):
  contrast = librosa.feature.spectral_contrast(y=y, sr=sr)
  print("contrast: ", len(contrast))
  return contrast

def extractMelSpectogram(y, sr):
  melspec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
  print("melspec: ", len(melspec))
  return melspec

def extractBPM(y, sr):
  tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
  onset_env = librosa.onset.onset_strength(y=y, sr=sr)
  tempo = librosa.feature.tempo(onset_envelope=onset_env, sr=sr)
  print("tempo: ", len(tempo))
  return tempo


df[['bpm', 'mfccs', 'rms', 'zcr', 'chroma_stft', 'spectral_centroid', 'spectral_bandwidth', 'spectral_rolloff', 'spectral_contrast', 'mel_spectogram', 'amplitude']] = df['path'].apply(lambda x: pd.Series(appendResults(x)))

df.iloc[df[(df.isnull().sum(axis=1) >= 1)].index]

"""## Extract all"""

def get_dominant_values(feature):
    # hdi_bounds = az.hdi(feature, hdi_prob=0.90)
    # lower, upper = hdi_bounds[0], hdi_bounds[1]
    # vals = (feature >= lower) & (feature <= upper)
    vals_mean = np.mean(feature)
    return vals_mean


def extract_features(data, sample_rate):

    result = np.array([])

    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40).T, axis=0)
    result = np.hstack((result, mfcc))
    # print("mfcc: ", len(mfcc))

    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate, hop_length=20, n_mels=128).T, axis=0)
    result = np.hstack((result, mel))
    # print("mel: ", len(mel))

    stft = np.abs(librosa.stft(data))
    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)
    result = np.hstack((result, chroma_stft))
    # print("chrome: ", len(chroma_stft))

    rolloff = librosa.feature.spectral_rolloff(y=data, sr=sample_rate)[0]
    result = np.hstack((result, get_dominant_values(rolloff)))
    # print("rolloff: ", len(rolloff))

    spec_bw = librosa.feature.spectral_bandwidth(y=data, sr=sample_rate)[0]
    result = np.hstack((result,get_dominant_values(spec_bw)))
    # print("spec_bw: ", len(spec_bw)

    contrast = librosa.feature.spectral_contrast(y=data)[0]
    result = np.hstack((result, get_dominant_values(contrast)))
    # print("contrast: ", len(contrast))

    flatness = librosa.feature.spectral_flatness(y=data)[0]
    result = np.hstack((result, get_dominant_values(flatness)))
    # print("flatness: ", len(flatness))

    cent = librosa.feature.spectral_centroid(y=data, sr=sample_rate)[0]
    result = np.hstack((result, get_dominant_values(cent)))

    rms =  librosa.feature.rms(y=data, frame_length=100)[0]
    result = np.hstack((result, get_dominant_values(rms)))
    # print("rms: ", len(rms))

    tempo, _ = librosa.beat.beat_track(y=data, sr=sample_rate)
    result = np.hstack((result, get_dominant_values(tempo)))
    # print("tempo: ", len(tempo))

    zcr = librosa.feature.zero_crossing_rate(y=data)[0]
    result = np.hstack((result, get_dominant_values(zcr)))
    print("zcr: ", len(zcr))

    return result

def feature_extractor(path):
    data, sample_rate = librosa.load(path, duration=25 ,sr=22050*2 ,offset=3)
    res1 = extract_features(data, sample_rate)
    result = np.array(res1)
    return result

p = feature_extractor(df['path'][0])
p.shape

#this is to remove data that has no audio file
for index, (data, emotion) in enumerate(zip(df["path"], df["label"])):
  if index % 20 == 0:
      print("#", end="")
  try:
    y, sr = librosa.load(data, duration=1 ,sr=10 ,offset=3)
  except:
    df.drop(df[df['path'] == data].index, inplace=True)
    continue

X, y = [], []
for index, (data, emotion) in enumerate(zip(df["path"], df["label"])):
    feature = feature_extractor(data)
    X.append(feature)
    y.append(emotion)
    if index % 20 == 0:
        print("#", end="")

X = np.array(X)
dataframe = pd.DataFrame(data = X[1:, 0:],
                         columns = X[0, 0:])
dataframe["label"] = y[1:]
dataframe.head()

dataframe.shape

# df = pd.concat([df,df_features], axis=1)
#naming: seconds_quality_sr_feature_stat
dataframe.to_csv('/content/drive/MyDrive/thesis/audio_features_25_soxrHQ_44_dominant_all_stacked_169.csv', index=False)
df_verify = pd.read_csv('/content/drive/MyDrive/thesis/audio_features_25_soxrHQ_44_dominant_all_stacked_169.csv')
print(df_verify)

"""# Data analysis"""

# file = '/content/drive/MyDrive/thesis/audio_features_25_soxrHQ_22.csv'
file = '/content/drive/MyDrive/thesis/audio_features_25_soxrHQ_44_dominant_all_stacked_169.csv'

df2 = pd.read_csv(file)

# df_second = pd.read_csv(file)

"""## Mean"""

audio_features = [
    (range(0, 40), 'MFCC'),
    (range(40, 168), 'Mel-Frequency'),
    (range(168, 180), 'Chroma'),
    (range(180, 181), 'Spectral Rolloff'),
    (range(181, 182), 'Spectral Bandwidth'),
    (range(182, 183), 'Spectral Contrast'),
    (range(183, 184), 'Spectral Flatness'),
    (range(184, 185), 'Spectral Centroid'),
    (range(185, 186), 'RMS'),
    (range(186, 187), 'Tempo'),
    (range(187, 188), 'ZCR'),
    (range(188, 189), 'Label'),
]
col_names = []
for feature_range, feature_name in audio_features:
    col_names.extend([f"{feature_name}_{i}" for i in feature_range])

df2.columns = col_names
df2

X = df2.rename(columns=lambda col: col.rsplit('_', 1)[0])
X = X.groupby(axis=1, level=0).mean()
X

sns.pairplot(X, hue="Label")

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
data = df2
X = df.drop(['path', 'label'], axis=1)  #independent columns
y = df['label']    #target column i.e price range
#apply SelectKBest class to extract top 10 best features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
bestfeatures = SelectKBest(score_func=chi2, k=10)
fit = bestfeatures.fit(X_scaled, y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)
#concat two dataframes for better visualization
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['feature','Score']  #naming the dataframe columns
print(featureScores.nlargest(11,'Score'))  #print 10 best features

X = df.drop(['path', 'label'], axis=1)  #independent columns
y = df['label']    #target column i.e price range
#get correlations of each features in dataset
corrmat = data.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap="RdYlGn")



"""## Mean + STD"""

file = '/content/drive/MyDrive/thesis/audio_features_25_soxrHQ_44_all_mean&std.csv'

df_second = pd.read_csv(file)

df_mean_std = df_second.drop(['path'], axis=1)
sns.pairplot(df_mean_std, hue="label")

"""## Max"""

file = '/content/drive/MyDrive/thesis/audio_features_25_soxrHQ_44_all_max.csv'

df_third = pd.read_csv(file)
df_max = df_third.drop(['path'], axis=1)
sns.pairplot(df_max, hue="label")



"""#Experiment"""

# file = '/content/drive/MyDrive/thesis/audio_features_25_soxrHQ_44_all_stacked.csv'
file = '/content/drive/MyDrive/thesis/audio_features_25_soxrHQ_44_all_stacked_189.csv'
df = pd.read_csv(file)
df.head()

df.columns = range(189)

X = np.array(df.drop(columns=df.iloc[:, :-1]))
y = np.array(df.iloc[:, -1])

df.iloc[:, -1].value_counts()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, stratify=y)

X_train.shape, y_train.shape

"""## SVM"""

from sklearn.svm import SVC
svc = SVC(probability = True,kernel='poly', verbose=2, random_state=2024, max_iter=-1)
fitSVC = svc.fit(X_train, y_train)
y_pred = fitSVC.predict(X_test)

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

param_grid = {'C': [0.1, 1, 10, 100, 1000],
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']}

from sklearn.model_selection import GridSearchCV
cv = GridSearchCV(SVC(random_state=2024), param_grid, refit = True, verbose = 3, scoring='roc_auc', n_jobs = -1)
cv.fit(X_train,y_train)

# Best Parameters
print(cv.best_params_)

print(cv.best_score_)

best_model = cv.best_estimator_
print(best_model)

"""## Neutral Network"""

from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()
y_train = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()
y_test = encoder.fit_transform(y_test.reshape(-1, 1)).toarray()

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
opt = Adam(learning_rate=0.001)

from keras import regularizers
model = Sequential()
model.add(Dense(75, activation="relu", input_shape=(186, )))
model.add(Dense(50, kernel_regularizer=regularizers.l2(0.01)))
model.add(Dense(50, activation="relu"))
model.add(Dense(50, activation="relu"))
model.add(Dense(4, activation="softmax"))
model.compile(optimizer=opt, loss="categorical_crossentropy", metrics=["accuracy"])

history = model.fit(
    x = X_train,
    y = y_train,
    validation_split = 0.2,
    epochs = 100,
    batch_size = 16
)

"""## K-NN"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
#####################################################################################

##### K- NN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=11, n_jobs = -1)
knn.fit( X_train , y_train )
y_pred = knn.predict(X_test)
y_pred_prob = knn.predict_proba(X_test)[:,1]

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

"""## Radius-Neighbor"""

from sklearn.neighbors import RadiusNeighborsClassifier
neigh = RadiusNeighborsClassifier(radius=1000.0, weights='distance')
neigh.fit(X, y)

y_pred = neigh.predict(X_test)
y_pred_prob = neigh.predict_proba(X_test)[:,1]

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(accuracy_score(y_test, y_pred))



"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=43)
clf.fit(X_train,y_train)

y_pred = clf.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(accuracy_score(y_test,y_pred))

from sklearn.model_selection import StratifiedKFold
kfold = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)

# param_grid = [
#   {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
#   {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
#  ]

param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
from sklearn.model_selection import GridSearchCV
clf = DecisionTreeClassifier(random_state=2018)
cv = GridSearchCV(clf, param_grid=param_grid, scoring='roc_auc', n_jobs = -1)

cv.fit(X,y)
# Best Parameters
print(cv.best_params_)

print(cv.best_score_)

best_model = cv.best_estimator_
print(best_model)

"""## Nearest mean"""

from sklearn.neighbors import NearestCentroid
clf = NearestCentroid()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))



"""## Linear Regression X Ridge classifier"""

from mlxtend.feature_selection import SequentialFeatureSelector
from sklearn.model_selection  import train_test_split
from sklearn.linear_model import LinearRegression

linearreg = LinearRegression()
forwad = SequentialFeatureSelector(
linearreg,
k_features=4,
forward=True,
verbose=1,
scoring="neg_mean_squared_error"
)
sf = forwad.fit(X_train,y_train)

from sklearn.linear_model import RidgeClassifier
alpha = .1  # Regularization strength (you can adjust this)
max_iter = 1000  # Maximum number of iterations for the solver to converge
solver = 'auto'  # Solver for optimization ('auto' chooses automatically)
tol = 1e-3
clf = RidgeClassifier(alpha=alpha, max_iter=max_iter, solver=solver, tol=tol).fit(X_train, y_train)
y_pred = clf.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

linearreg = RidgeClassifier(alpha=alpha, max_iter=max_iter, solver=solver, tol=tol)
forwad = SequentialFeatureSelector(
linearreg,
k_features=4,
forward=True,
verbose=1,
scoring="neg_mean_squared_error"
)
sf = forwad.fit(X_train,y_train)

X_train

feat_names = list(sf.k_feature_names_)
print(feat_names)

print(sf.k_feature_idx_)

y = df.iloc[:, -1]   # The last column (target)
y.head()

from sklearn.tree import DecisionTreeClassifier
X = df.iloc[:, -9:-1]  # All columns except the last one (features)

y = df.iloc[:, -1]   # The last column (target)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize an empty dictionary to store results
results = {}

# Iterate over each feature (column)
for i, feature in enumerate(X.columns):
    # Select a single feature for training
    X_train_single = X_train[[feature]]
    X_test_single = X_test[[feature]]

    # Train a Decision Tree Classifier
    clf = DecisionTreeClassifier(random_state=42)
    clf.fit(X_train_single, y_train)

    # Make predictions
    y_pred = clf.predict(X_test_single)

    # Evaluate accuracy
    accuracy = accuracy_score(y_test, y_pred)

    # Store the result
    results[feature] = accuracy

# Display results
for feature, accuracy in results.items():
    print(f"Feature: {feature}, Accuracy: {accuracy:.4f}")

X_mfcc = df.iloc[:, :40]

X_train_mfcc, X_test_mfcc, y_train_mfcc, y_test_mfcc = train_test_split(X_mfcc, y, test_size=0.2, random_state=42)
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train_mfcc, y_train_mfcc)

# Make predictions
y_pred_mfcc = clf.predict(X_test_mfcc)

# Evaluate accuracy
accuracy = accuracy_score(y_test_mfcc, y_pred_mfcc)
print(f"Accuracy with MFCC features: {accuracy:.4f}")

X_mel = df.iloc[:, 40:168]

X_train_mel, X_test_mel, y_train_mel, y_test_mel = train_test_split(X_mel, y, test_size=0.2, random_state=42)
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train_mel, y_train_mel)

# Make predictions
y_pred_mel = clf.predict(X_test_mel)

# Evaluate accuracy
accuracy = accuracy_score(y_test_mel, y_pred_mel)
print(f"Accuracy with mel features: {accuracy:.4f}")

X_chroma = df.iloc[:, 168:180]
X_train_chroma, X_test_chroma, y_train_chroma, y_test_chroma = train_test_split(X_chroma, y, test_size=0.2, random_state=42)
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train_chroma, y_train_chroma)

# Make predictions
y_pred_chroma = clf.predict(X_test_chroma)

# Evaluate accuracy
accuracy = accuracy_score(y_test_chroma, y_pred_chroma)
print(f"Accuracy with chroma features: {accuracy:.4f}")

X_roll = df.iloc[:, 180:181]
X_train_roll, X_test_roll, y_train_roll, y_test_roll = train_test_split(X_roll, y, test_size=0.2, random_state=42)
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train_roll, y_train_roll)

# Make predictions
y_pred_roll = clf.predict(X_test_roll)

# Evaluate accuracy
accuracy = accuracy_score(y_test_roll, y_pred_roll)
print(f"Accuracy with roll features: {accuracy:.4f}")

import matplotlib.pyplot as plt

# Feature names and their corresponding accuracy values
features = ['MFCC', 'Mel', 'Chroma', 'Rolloff', 'Bandwidth', 'Contrast', 'Flatness', 'Centroid', 'RMS', 'Tempo', 'ZCR']
accuracy_values = [0.5385, 0.5000, 0.6154, 0.3462, 0.1154, 0.4231, 0.4615, 0.5385, 0.3077, 0.2692, 0.4231]

# Create a bar plot
plt.bar(features, accuracy_values, color='skyblue')

# Add labels and title
plt.xlabel('Features')
plt.ylabel('Accuracy')
plt.title('Accuracy of Different Features Using Decision Tree')
plt.xticks(rotation=45)
# Show the plot
plt.show()



"""# Segmentation"""

path_audio_full = '/content/drive/MyDrive/thesis/audio/songofruth.mp3'

def segment_audio(audio, sr, seconds, overlap):
    if seconds <= 0 or overlap < 0 or seconds <= overlap:
        raise ValueError("Invalid seconds or overlap parameters.")
    seconds = int(seconds * sr)
    overlap = int(overlap * sr)
    step = seconds - overlap
    segments = []

    for i in range(0, len(audio) - seconds + 1, step):
        segments.append(audio[i:i + seconds])

    # Optional: Include the last partial segment if needed
    # if (len(audio) - seconds) % step != 0 and len(audio) > seconds:
    #     segments.append(audio[-seconds:])

    return segments



def getSegments():

    y, sr = librosa.load(path_audio_full ,sr=22050*2)
    y, _ = librosa.effects.trim(y)
    # y = np.arange(401000 * 25)  # Example signal
    # sr = 40100               # 22 kHz sampling rate
    seconds = 5       # seconds per segment
    overlap = 2   # seconds overlap
    print(len(y))
    return segment_audio(y, sr, seconds, overlap)

segments = getSegments()
segments

X = []
for i, data in enumerate(segments):
    segment = extract_features(data, 22050 * 2)
    X.append(segment)

X = np.array(X)
dataframe = pd.DataFrame(data = X, columns = [str(x) for x in range(1, X.shape[1] + 1)])
dataframe.head()

dataframe.shape

#predict: call to predict a segment

X_predict = []

# | code | predicted |
step = 5 # the seconds
for i in range(0, len(df), step):
  song = df.iloc[i:i + step]
  for segment in song:
    result = 0
    print(segment)
    X_predict.append(result)


X_predict = np.array(X_predict)
dataframe = pd.DataFrame(data = X, columns = ['code', 'predicted'])
dataframe.head()

path = '/content/drive/MyDrive/thesis/dataset/songofruth_segments.csv'
with open(path, 'w', encoding = 'utf-8-sig') as f:
  dataframe.to_csv(f)